{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import itertools\n",
    "from itertools  import product\n",
    "from itertools  import combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS // THAT DONT CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Telegram Bot   ###################################\n",
    "\n",
    "import requests\n",
    "\n",
    "def telegram_bot_sendtext(bot_message):\n",
    "    \n",
    "    bot_token = '2062474091:AAGp1GiSrNw7DRds4qwLHBOkZ_Do9HlQ5V8'\n",
    "    bot_chatID = '2013533042'\n",
    "    send_text = 'https://api.telegram.org/bot' + bot_token + '/sendMessage?chat_id=' + bot_chatID + '&parse_mode=Markdown&text=' + bot_message\n",
    "    \n",
    "    response = requests.get(send_text)\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "########################## Functions   ###################################\n",
    "\n",
    "def readdata(location):\n",
    "    \n",
    "    data=pd.read_csv(location)\n",
    "    Greece= data[data.location =='Greece'].reset_index(drop='True')\n",
    "    Greece = Greece.dropna(how='all', axis=1)\n",
    "    Greece_total = Greece.iloc[7:498, 3:40].reset_index(drop='True')\n",
    "    titles =Greece_total.columns\n",
    "    return Greece_total , titles\n",
    "\n",
    "\n",
    "\n",
    "def featcombos(featurename ,titles , combin) :\n",
    "    \n",
    "    titles.str.contains(featurename)\n",
    "    features = titles[titles.str.contains(featurename)].to_list()\n",
    "    print(features)\n",
    "    feature_list = list(combinations(features , combin))\n",
    "    \n",
    "    return feature_list\n",
    "\n",
    "def createdata(dataset ,features):\n",
    "    \n",
    "    Greece=dataset[features]\n",
    "    Greece[\"date\"] = Greece_total['date']\n",
    "    Greece=Greece.dropna(axis=0)\n",
    "    \n",
    "    dates=pd.DataFrame(Greece['date']).reset_index(drop=True)\n",
    "    Greece=Greece[(features)].reset_index(drop=True)\n",
    "    \n",
    "    return dates , Greece\n",
    "\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def split_data(data, sequence):\n",
    "    train_set = data[:355].reset_index(drop=True)\n",
    "    validation_set = data[355 - sequence:369].reset_index(drop=True)\n",
    "    test_set = data[369 - sequence:].reset_index(drop=True) \n",
    "    \n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "\n",
    "def timeseries_gen(seq_size, n_features, train, val, test):\n",
    "    # Train Set\n",
    "    train_generator = TimeseriesGenerator(train, train.iloc[:, 0], length=seq_size, batch_size=1)\n",
    "    print(\"Total number of samples in the original training data = \", len(train))\n",
    "    print(\"Total number of samples in the generated training data = \", len(train_generator))\n",
    "\n",
    "\n",
    "    # Validation Set\n",
    "    val_generator = TimeseriesGenerator(val, val.iloc[:, 0], length=seq_size, batch_size=1)\n",
    "    print(\"Total number of samples in the original validation data = \", len(val))\n",
    "    print(\"Total number of samples in the validation data = \", len(val_generator))\n",
    "\n",
    "    # Test Set\n",
    "    test_generator = TimeseriesGenerator(test, test.iloc[:, 0], length=seq_size, batch_size=1)\n",
    "    print(\"Total number of samples in the original test data = \", len(test))\n",
    "    print(\"Total number of samples in the generated test data = \", len(test_generator))\n",
    "    return train_generator, val_generator, test_generator\n",
    "\n",
    "def plotloss(mod, name=\"\"):\n",
    "    plt.figure(figsize=[12,10] , dpi=140 )\n",
    "    loss = mod.history['loss']\n",
    "    val_loss = mod.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "#     plt.savefig(\"Plots\\loss_model\" + name +\".jpeg\"  )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotprediction(ypredict , name=\"\"):\n",
    "    plt.figure(figsize=[12,10] , dpi=140 )\n",
    "    plt.plot(ypredict.index, ypredict.iloc[:, 0], 'y', label='Prediction ')\n",
    "    plt.plot(ypredict.index, ypredict.iloc[:, 1], 'r', label='Actual ')\n",
    "    plt.title('Predicted vs  Actual Cases in Greece for ' +str(len(ypredict)) + ' days')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cases')\n",
    "    plt.legend()\n",
    "#     plt.savefig(\"Plots\\pred\" + name +\".jpeg\"  )\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def inversesets(sequence,feature_list, sc, trainset, validationset, testset, ogdata, dates):\n",
    "    \n",
    "    drange =dates.loc[0]\n",
    "    drange=pd.to_datetime(drange[\"date\"])\n",
    "    date_index = pd.date_range(drange , periods=len(dates), freq='D')\n",
    "\n",
    "    \n",
    "    \n",
    "    set1 = pd.DataFrame(sc.inverse_transform(trainset),index=date_index[0:len(trainset)])\n",
    "\n",
    "    set1=set1.set_axis(feature_list, axis=1, inplace=False)\n",
    "    \n",
    "    set2 = pd.DataFrame(sc.inverse_transform(validationset),index=date_index[len(trainset) - sequence:len(trainset) + len(validationset) - sequence])\n",
    "    set2=set2.set_axis(feature_list, axis=1, inplace=False)\n",
    "\n",
    "    set3 = pd.DataFrame(sc.inverse_transform(testset),index=date_index[-len(testset):])\n",
    "    set3=set3.set_axis(feature_list, axis=1, inplace=False)\n",
    "    return set1, set2, set3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "MODEL"
    ]
   },
   "source": [
    "MODEL CREATION , TRAINING , PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_create(nodes, seq_size , features,lrate):\n",
    "    opt = keras.optimizers.Adam(learning_rate=lrate)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(44, activation='relu', return_sequences=False, input_shape=(seq_size, features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "TRAIN"
    ]
   },
   "source": [
    "TRAIN NO PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(i, model, traingenerator, valgenerator, ep):\n",
    "    history = model.fit(traingenerator, validation_data=valgenerator, epochs=ep, verbose=1)\n",
    "    model.save('Models\\model_' + str(i) + '.h5', overwrite=True)\n",
    "#     plotloss(history,str(i))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sc, valgenerator, validation_set, inverseval, trainset ):\n",
    "    \n",
    "\n",
    "    # Forecast   Predict using a for loop\n",
    "    index = inverseval.index\n",
    "    \n",
    "    predictiondata = pd.DataFrame(inverseval[:seq_size])  # Empty list to populate later with predictions\n",
    "    predictiondata = pd.DataFrame(trainset[-seq_size:]).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    A=[\t0.4499396863691194, 0.4469240048250904, 0.39897466827503014, 0.41375150784077197,0.36851628468033776, 0.19963811821471653]\n",
    "    newcasesprediction = pd.DataFrame(A)\n",
    "    \n",
    "    current_batch = trainset[-seq_size:]\n",
    "    forecast = pd.DataFrame()\n",
    "\n",
    "    # Predict future, beyond test dates\n",
    "    future = len(validation_set) - seq_size  # Days\n",
    "    for i in range(future): #instead of future\n",
    "        \n",
    "        current_batch = predictiondata[i:seq_size + i] #Create input for LSTM (Based on sequence size )\n",
    "        current_batch = current_batch.to_numpy()  #Input to array \n",
    "        current_batch = current_batch.reshape(1, seq_size, n_features)  # Reshape\n",
    "\n",
    "        ### Prediction ##\n",
    "        \n",
    "        current_pred = model.predict(current_batch) # Make a prediction \n",
    "        current_pred = float(current_pred[0]) #Convert Prediction to integer \n",
    "        \n",
    "        \n",
    "        \n",
    "        # ##### Create New Day Values #####\n",
    "        \n",
    "        #### Total Cases ####\n",
    "        per_mil_tot = current_pred * 0.096 #Calculate Total Caces per million \n",
    "        \n",
    "        #### New Cases ####\n",
    "        new_cases= current_pred-predictiondata.iloc[len(predictiondata.index)-1,0] # Calculate  new cases         \n",
    "        newcasesprediction.loc[len(newcasesprediction.index)] = [new_cases] #append new cases \n",
    "        \n",
    "        per_mil_new = new_cases*0.096  #Calculate New per million \n",
    "        \n",
    "        \n",
    "        smoothednew = newcasesprediction.rolling(window=7).mean()\n",
    "        smoothednew = float( smoothednew.iloc[6+i])\n",
    "        per_mil_smoothed_new= smoothednew * 0.096  #Calculate Smoothed Permillion New Cases \n",
    "        \n",
    "        # print(\"\\n ******************** \\n\")\n",
    "        # print(current_pred)\n",
    "        # print(new_cases)\n",
    "        # print(smoothednew)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Add New Day Values \n",
    "        predictiondata.loc[len(predictiondata.index)] = [current_pred , new_cases]\n",
    "                                                    # Fill the two first collumns of the Dataframe \n",
    "        # print(predictiondata)\n",
    "\n",
    "        # predictiondata['Percentage'] = predictiondata['Daily_Confirmed_Cases'].pct_change() #Calculate Percentage \n",
    "        # predictiondata['Moving Average'] = predictiondata[\"New Cases\"].rolling(3).mean() #Calculate Mean \n",
    "        # predictiondata=predictiondata.fillna(0.0051519) # Fill one missing value with the true value \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    forecast = predictiondata[-(future):] #Save results in a dataframe \n",
    "    forecast = sc.inverse_transform(forecast)#Inverse Transform to get the actual cases \n",
    "    forecast = pd.DataFrame(forecast.round()) #Round results \n",
    "    forecast = forecast.set_index(index[seq_size:], 'Date').rename(columns={0: 'Prediction'})\n",
    "\n",
    "    forecast = pd.concat([forecast['Prediction'], inverseval['total_cases'][seq_size:]], axis=1 ,ignore_index=True) #Concate the two dfs \n",
    "\n",
    "    forecast=forecast.set_axis(['Prediction', 'Actual'], axis=1, inplace=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIND BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(mape):\n",
    "    mape = pd.DataFrame(mape)\n",
    "    min = mape.idxmin()\n",
    "    j = min[0]\n",
    "    best_model = keras.models.load_model(r\"Models/model_\" + str(j) + \".h5\")\n",
    "    print(\"Best Model is :model_\" + str(j) + \".h5\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLUSTER FUCK LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments(times, nodes, scaler, seq_size, epochs, n_features, train_generator, val_generator, validation_set,\n",
    "                train_set, inv_val, inv_test, dates ,lrate):\n",
    "    \n",
    "    experimentmodel = model_create(nodes, seq_size ,n_features , lrate)\n",
    "\n",
    "    experimentmodel = model_train(i, experimentmodel, train_generator, val_generator, epochs)  # Train Model\n",
    "\n",
    "    forecast = predict(experimentmodel, scaler, val_generator, validation_set, inv_val, train_set)\n",
    "#     plotprediction(forecast ,str(i))\n",
    "    \n",
    "    \n",
    "    ##################### Metrics ######################\n",
    "\n",
    "    mae_4 = mean_absolute_error(forecast['Actual'], forecast['Prediction'])\n",
    "    MAE_4.append(mae_4)\n",
    "\n",
    "    mape_4 = mean_absolute_percentage_error(forecast['Actual'], forecast['Prediction'])\n",
    "    MAPE_4.append(mape_4)\n",
    "\n",
    "    mse_4 = mean_squared_error(forecast['Actual'], forecast['Prediction'])\n",
    "    MSE_4.append(mse_4)\n",
    "\n",
    "    rmse_4 = mean_squared_error(forecast['Actual'], forecast['Prediction'], squared=False)\n",
    "    RMSE_4.append(rmse_4)\n",
    "\n",
    "    node.append(nodes)\n",
    "\n",
    "\n",
    "    mape_4_next_day = mean_absolute_percentage_error(forecast['Actual'][:1], forecast['Prediction'][:1])\n",
    "    MAPE_4_Next_day.append(mape_4_next_day)\n",
    " \n",
    "    mape_3days = mean_absolute_percentage_error(forecast['Actual'][:3], forecast['Prediction'][:3])\n",
    "    MAPE_4_3days.append(mape_3days)\n",
    "    \n",
    "    mape_7days = mean_absolute_percentage_error(forecast['Actual'][:7], forecast['Prediction'][:7])\n",
    "    MAPE_4_7days.append(mape_7days)\n",
    "    \n",
    "    Epochs.append(epochs)\n",
    "    LR.append(lrate)\n",
    "        \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hyper(parameter1 , parameter2 , parameter3 , repetitions):\n",
    "    hp1 = list(product(parameter1 , parameter2 ))\n",
    "    Hyperparameters = list (product(hp1 , parameter3))\n",
    "    Hyperparameters= pd.DataFrame(Hyperparameters).rename(columns={0: \"A\", 1: \"Nodes\"})\n",
    "    \n",
    "    Hyperparameters[['Learning Rate' , 'Epochs']]= pd.DataFrame(Hyperparameters['A'].tolist(), index=Hyperparameters.index)\n",
    "    \n",
    "    Hyperparameters =Hyperparameters.drop(['A'], axis=1)\n",
    "    Hyperparameters=Hyperparameters.sort_values(by=['Nodes', 'Learning Rate' ,'Epochs' ])\n",
    "    Hyperparameters=pd.concat([Hyperparameters]*times)\n",
    "    \n",
    "    \n",
    "    Hyperparameters= list(Hyperparameters.itertuples(index=False, name=None))\n",
    "    \n",
    "    \n",
    "    return Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMTERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_size = 3\n",
    "times =10\n",
    "combos=2\n",
    "nodes=2\n",
    "lr = 0.0001\n",
    "epochs=75\n",
    "\n",
    "\n",
    "\n",
    "loc=\"owid-covid-data.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN LOOP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['total_cases', 'new_cases', 'new_cases_smoothed', 'total_cases_per_million', 'new_cases_per_million', 'new_cases_smoothed_per_million']\n",
      "Total number of samples in the original training data =  355\n",
      "Total number of samples in the generated training data =  352\n",
      "Total number of samples in the original validation data =  17\n",
      "Total number of samples in the validation data =  14\n",
      "Total number of samples in the original test data =  125\n",
      "Total number of samples in the generated test data =  122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arisdourdounas/opt/anaconda3/envs/pythonProject/lib/python3.6/site-packages/ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 44)                8272      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 45        \n",
      "=================================================================\n",
      "Total params: 8,317\n",
      "Trainable params: 8,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train for 352 steps, validate for 14 steps\n",
      "Epoch 1/75\n",
      "352/352 [==============================] - 3s 9ms/step - loss: 0.1255 - val_loss: 0.6602\n",
      "Epoch 2/75\n",
      "352/352 [==============================] - 2s 5ms/step - loss: 0.0708 - val_loss: 0.3346\n",
      "Epoch 3/75\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.0344 - val_loss: 0.1241\n",
      "Epoch 4/75\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.0160 - val_loss: 0.0357\n",
      "Epoch 5/75\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.0071 - val_loss: 0.0074\n",
      "Epoch 6/75\n",
      "352/352 [==============================] - 1s 4ms/step - loss: 0.0034 - val_loss: 0.0031\n",
      "Epoch 7/75\n",
      "221/352 [=================>............] - ETA: 0s - loss: 0.0017"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-64325c82d09c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# nodes , lr , epochs = Hyperparameters[i]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     experiments(i, nodes, scaler, seq_size, epochs, n_features, train_generator, val_generator,\n\u001b[0;32m---> 68\u001b[0;31m                       validation_set, train_set, inv_val, inv_test, dates , lr )\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-c3a7b3e60ba6>\u001b[0m in \u001b[0;36mexperiments\u001b[0;34m(times, nodes, scaler, seq_size, epochs, n_features, train_generator, val_generator, validation_set, train_set, inv_val, inv_test, dates, lrate)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mexperimentmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_size\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_features\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mexperimentmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperimentmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mforecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperimentmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-b6e84b850a23>\u001b[0m in \u001b[0;36mmodel_train\u001b[0;34m(i, model, traingenerator, valgenerator, ep)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraingenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraingenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Models\\model_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     plotloss(history,str(i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Greece_total , titles =readdata(loc)\n",
    "flist = featcombos('cases', titles, combos)\n",
    "\n",
    "\n",
    "feature_list =flist[0]\n",
    "feature_list = list(itertools.chain(feature_list))\n",
    "n_features = len(feature_list)\n",
    "\n",
    "Greece_total['new_cases_smoothed']= Greece_total['new_cases'].rolling(window=7).mean()\n",
    "Greece_total['new_deaths_smoothed']= Greece_total['new_deaths'].rolling(window=7).mean()\n",
    "\n",
    "Greece_total['new_cases_smoothed_per_million']= Greece_total['new_cases_smoothed']*0.096\n",
    "Greece_total['new_deaths_smoothed_per_million']= Greece_total['new_deaths_smoothed']*0.096\n",
    "\n",
    "\n",
    "\n",
    "dates,greece = createdata(Greece_total ,feature_list )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_set, validation_set, test_set = split_data( greece, seq_size)\n",
    "\n",
    "#Scaling \n",
    "scaler = MinMaxScaler() \n",
    "scaler.fit(train_set)\n",
    "train_set=pd.DataFrame(scaler.transform(train_set))\n",
    "train_set=train_set.set_axis(feature_list, axis=1, inplace=False)\n",
    "\n",
    "validation_set=pd.DataFrame(scaler.transform(validation_set))\n",
    "validation_set=validation_set.set_axis(feature_list, axis=1, inplace=False)\n",
    "\n",
    "test_set=pd.DataFrame(scaler.transform(test_set))\n",
    "test_set=test_set.set_axis(feature_list, axis=1, inplace=False)\n",
    "\n",
    "\n",
    "train_generator, val_generator, test_generator = timeseries_gen(seq_size, n_features, train_set, validation_set,\n",
    "                                                                test_set)\n",
    "a =train_generator[3]\n",
    "\n",
    "inv_train, inv_val, inv_test = inversesets(seq_size,feature_list, scaler, train_set, validation_set, test_set, greece,\n",
    "                                                        dates)\n",
    "\n",
    "Epochs = []\n",
    "LR = []\n",
    "node = []\n",
    "MAE_4 = []\n",
    "MAPE_4 = []\n",
    "MSE_4 = []\n",
    "RMSE_4 = []\n",
    "MAPE_4_3days = []\n",
    "MAPE_4_7days = []\n",
    "MAPE_4_Next_day = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters= Hyper(learning_rate, epochs, nodes ,times )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range(1):\n",
    "    # nodes , lr , epochs = Hyperparameters[i]\n",
    "    experiments(i, nodes, scaler, seq_size, epochs, n_features, train_generator, val_generator,\n",
    "                      validation_set, train_set, inv_val, inv_test, dates , lr )\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "hours, rem = divmod(end - start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours), int(minutes), seconds))\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    {'MAE_4': MAE_4, 'MAPE_4 1 Day': MAPE_4_Next_day,\n",
    "      'MAPE_4 3 Days': MAPE_4_3days,'MAPE_4 7 days': MAPE_4_7days, 'MAPE_4': MAPE_4, 'MSE_4': MSE_4, 'RMSE_4': RMSE_4, 'Nodes': node , 'Learning Rate' : LR , 'Epochs' : Epochs})\n",
    "\n",
    "metrics =metrics.append( metrics.groupby(['Nodes' , 'Learning Rate'  , 'Epochs']).mean())\n",
    "# metrics = metrics.groupby(['Nodes' , 'Learning Rate'  , 'Epochs']).mean()\n",
    "\n",
    "\n",
    "\n",
    "#Save Results\n",
    "metrics.to_csv(\"Results/Valdation_Results_for_\"+ str(feature_list) +\".csv\", float_format=\"%.5f\",index=True, header=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "bestmodel = find_best_model(MAPE_4)\n",
    "\n",
    "\n",
    "bestmodel.fit_generator(val_generator, epochs=30, verbose=1) \n",
    "bestmodel.save(r\"Models\\Final_model_for_\"+ str(feature_list) + \".h5\")\n",
    "\n",
    "forecastf = predict(bestmodel, scaler, test_generator, test_set, inv_test, validation_set )\n",
    "\n",
    "plotprediction(forecastf[:7] , \"iction_7_day_prediction\")\n",
    "plotprediction(forecastf[:14] , \"iction_14_day_prediction\")\n",
    "plotprediction(forecastf[:30] , \"iction_30_day_prediction\")\n",
    "plotprediction(forecastf[:60] , \"iction_60_day_prediction\")\n",
    "plotprediction(forecastf[:90] , \"iction_90_day_prediction\")\n",
    "\n",
    "\n",
    "mae = mean_absolute_error(forecastf['Actual'], forecastf['Prediction'])\n",
    "mae= float(\"{:.3f}\".format(mae))\n",
    "\n",
    "mape = mean_absolute_percentage_error(forecastf['Actual'], forecastf['Prediction'])\n",
    "mape= float(\"{:.3f}\".format(mape))\n",
    "\n",
    "mape_1day = mean_absolute_percentage_error(forecastf['Actual'][:1], forecastf['Prediction'][:1])\n",
    "mape_1day= float(\"{:.3f}\".format(mape_1day))\n",
    "\n",
    "\n",
    "mape_3days = mean_absolute_percentage_error(forecastf['Actual'][:3], forecastf['Prediction'][:3])\n",
    "mape_3days= float(\"{:.3f}\".format(mape_3days))\n",
    "\n",
    "mape_7days = mean_absolute_percentage_error(forecastf['Actual'][:7], forecastf['Prediction'][:7])\n",
    "mape_7days= float(\"{:.3f}\".format(mape_7days))\n",
    "                  \n",
    "mape_14days = mean_absolute_percentage_error(forecastf['Actual'][:14], forecastf['Prediction'][:14])\n",
    "mape_14days= float(\"{:.3f}\".format(mape_14days))\n",
    "\n",
    "mape_30days = mean_absolute_percentage_error(forecastf['Actual'][:30], forecastf['Prediction'][:30])\n",
    "mape_30days= float(\"{:.3f}\".format(mape_30days))\n",
    "\n",
    "mape_60days = mean_absolute_percentage_error(forecastf['Actual'][:60], forecastf['Prediction'][:60])\n",
    "mape_60days= float(\"{:.3f}\".format(mape_60days))\n",
    "\n",
    "mse = mean_squared_error(forecastf['Actual'], forecastf['Prediction'])\n",
    "mse= float(\"{:.3f}\".format(mse))\n",
    "rmse = mean_squared_error(forecastf['Actual'], forecastf['Prediction'], squared=False)\n",
    "rmse= float(\"{:.3f}\".format(rmse))\n",
    "\n",
    "finalresults=pd.DataFrame({\"MAE\": [mae],\"MAPE 1 Day\" : [mape_1day] , \"MAPE 3 Days\" :[mape_3days],\"MAPE 7 Days \" :[mape_7days] , \"MAPE 14 Days\" :[mape_14days], \"MAPE 30 Days\" :[mape_30days],\"MAPE 60 Days\" :[mape_60days],\"MAPE\":[mape], \"RMSE\": [rmse], \"MSE\":[mse]})\n",
    "\n",
    "\n",
    "finalresults.to_csv(\"Results\\Final_Results_for_\" + str(feature_list) +\".csv\", float_format=\"%.3f\",index=True, header=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7456b8b16b80f58bc66af7790c179bcda12a832a7c60efc6ab6015df57e8528d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('pythonProject': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
