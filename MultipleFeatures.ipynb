{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import itertools\n",
    "from itertools  import product\n",
    "from itertools  import combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS // THAT DONT CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Telegram Bot   ###################################\n",
    "\n",
    "import requests\n",
    "\n",
    "def telegram_bot_sendtext(bot_message):\n",
    "    \n",
    "    bot_token = '2062474091:AAGp1GiSrNw7DRds4qwLHBOkZ_Do9HlQ5V8'\n",
    "    bot_chatID = '2013533042'\n",
    "    send_text = 'https://api.telegram.org/bot' + bot_token + '/sendMessage?chat_id=' + bot_chatID + '&parse_mode=Markdown&text=' + bot_message\n",
    "    \n",
    "    response = requests.get(send_text)\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "########################## Functions   ###################################\n",
    "\n",
    "def readdata(location):\n",
    "    \n",
    "    data=pd.read_csv(location)\n",
    "    Greece= data[data.location =='Greece'].reset_index(drop='True')\n",
    "    Greece = Greece.dropna(how='all', axis=1)\n",
    "    Greece_total = Greece.iloc[7:498, 3:40].reset_index(drop='True')\n",
    "    titles =Greece_total.columns\n",
    "    return Greece_total , titles\n",
    "\n",
    "\n",
    "\n",
    "def featcombos(featurename ,titles , combin) :\n",
    "    \n",
    "    titles.str.contains(featurename)\n",
    "    features = titles[titles.str.contains(featurename)].to_list()\n",
    "    print(features)\n",
    "    feature_list = list(combinations(features , combin))\n",
    "    \n",
    "    return feature_list\n",
    "\n",
    "def createdata(dataset ,features):\n",
    "    \n",
    "    Greece=dataset[features]\n",
    "    Greece[\"date\"] = Greece_total['date']\n",
    "    Greece=Greece.dropna(axis=0)\n",
    "    \n",
    "    dates=pd.DataFrame(Greece['date']).reset_index(drop=True)\n",
    "    Greece=Greece[(features)].reset_index(drop=True)\n",
    "    \n",
    "    return dates , Greece\n",
    "\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def split_data(data, sequence):\n",
    "    train_set = data[:355].reset_index(drop=True)\n",
    "    validation_set = data[355 - sequence:369].reset_index(drop=True)\n",
    "    test_set = data[369 - sequence:].reset_index(drop=True) \n",
    "    \n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "\n",
    "def timeseries_gen(seq_size, n_features, train, val, test):\n",
    "    # Train Set\n",
    "    train_generator = TimeseriesGenerator(train, train.iloc[:, 0], length=seq_size, batch_size=1)\n",
    "    print(\"Total number of samples in the original training data = \", len(train))\n",
    "    print(\"Total number of samples in the generated training data = \", len(train_generator))\n",
    "\n",
    "\n",
    "    # Validation Set\n",
    "    val_generator = TimeseriesGenerator(val, val.iloc[:, 0], length=seq_size, batch_size=1)\n",
    "    print(\"Total number of samples in the original validation data = \", len(val))\n",
    "    print(\"Total number of samples in the validation data = \", len(val_generator))\n",
    "\n",
    "    # Test Set\n",
    "    test_generator = TimeseriesGenerator(test, test.iloc[:, 0], length=seq_size, batch_size=1)\n",
    "    print(\"Total number of samples in the original test data = \", len(test))\n",
    "    print(\"Total number of samples in the generated test data = \", len(test_generator))\n",
    "    return train_generator, val_generator, test_generator\n",
    "\n",
    "def plotloss(mod, name=\"\"):\n",
    "    plt.figure(figsize=[12,10] , dpi=140 )\n",
    "    loss = mod.history['loss']\n",
    "    val_loss = mod.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "#     plt.savefig(\"Plots\\loss_model\" + name +\".jpeg\"  )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotprediction(ypredict , name=\"\"):\n",
    "    plt.figure(figsize=[12,10] , dpi=140 )\n",
    "    plt.plot(ypredict.index, ypredict.iloc[:, 0], 'y', label='Prediction ')\n",
    "    plt.plot(ypredict.index, ypredict.iloc[:, 1], 'r', label='Actual ')\n",
    "    plt.title('Predicted vs  Actual Cases in Greece for ' +str(len(ypredict)) + ' days')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cases')\n",
    "    plt.legend()\n",
    "#     plt.savefig(\"Plots\\pred\" + name +\".jpeg\"  )\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def inversesets(sequence,feature_list, sc, trainset, validationset, testset, ogdata, dates):\n",
    "    \n",
    "    drange =dates.loc[0]\n",
    "    drange=pd.to_datetime(drange[\"date\"])\n",
    "    date_index = pd.date_range(drange , periods=len(dates), freq='D')\n",
    "\n",
    "    \n",
    "    \n",
    "    set1 = pd.DataFrame(sc.inverse_transform(trainset),index=date_index[0:len(trainset)])\n",
    "\n",
    "    set1=set1.set_axis(feature_list, axis=1, inplace=False)\n",
    "    \n",
    "    set2 = pd.DataFrame(sc.inverse_transform(validationset),index=date_index[len(trainset) - sequence:len(trainset) + len(validationset) - sequence])\n",
    "    set2=set2.set_axis(feature_list, axis=1, inplace=False)\n",
    "\n",
    "    set3 = pd.DataFrame(sc.inverse_transform(testset),index=date_index[-len(testset):])\n",
    "    set3=set3.set_axis(feature_list, axis=1, inplace=False)\n",
    "    return set1, set2, set3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "MODEL"
    ]
   },
   "source": [
    "MODEL CREATION , TRAINING , PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_create(nodes, seq_size , features,lrate):\n",
    "    # opt = keras.optimizers.Adam(learning_rate=lrate)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(44, activation='relu', return_sequences=False, input_shape=(seq_size, features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "TRAIN"
    ]
   },
   "source": [
    "TRAIN NO PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(i, model, traingenerator, valgenerator, ep):\n",
    "    history = model.fit(traingenerator, validation_data=valgenerator, epochs=ep, verbose=1)\n",
    "    model.save('Models\\model_' + str(i) + '.h5', overwrite=True)\n",
    "#     plotloss(history,str(i))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sc, valgenerator, validation_set, inverseval, trainset ):\n",
    "    \n",
    "\n",
    "    # Forecast   Predict using a for loop\n",
    "    index = inverseval.index\n",
    "    \n",
    "    predictiondata = pd.DataFrame(inverseval[:seq_size])  # Empty list to populate later with predictions\n",
    "    predictiondata = pd.DataFrame(trainset[-seq_size:]).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    A=[\t1492, 1482, 1323, 1372, 1222, 662]\n",
    "    newcasesprediction = pd.DataFrame(A)\n",
    "    \n",
    "    current_batch = trainset[-seq_size:]\n",
    "    forecast = pd.DataFrame()\n",
    "\n",
    "    # Predict future, beyond test dates\n",
    "    future = len(validation_set) - seq_size  # Days\n",
    "    for i in range(future): #instead of future\n",
    "        \n",
    "        current_batch = predictiondata[i:seq_size + i] #Create input for LSTM (Based on sequence size )\n",
    "        current_batch = current_batch.to_numpy()  #Input to array \n",
    "        current_batch = current_batch.reshape(1, seq_size, n_features)  # Reshape\n",
    "\n",
    "        ### Prediction ##\n",
    "        \n",
    "        current_pred = model.predict(current_batch) # Make a prediction \n",
    "        total_cases = float(current_pred[0]) #Convert Prediction to integer \n",
    "        total_cases= total_cases /5.80966e-06 #De-scale\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ##### Create New Day Values #####\n",
    "        \n",
    "        #### Total Cases ####\n",
    "        \n",
    "        total_cases_per_million = total_cases * 0.096 #Calculate Total Caces per million \n",
    "        \n",
    "        #### New Cases ####\n",
    "        \n",
    "        new_cases= total_cases-(predictiondata.iloc[len(predictiondata.index)-1,0])/5.80966e-06 # Calculate  new casesDe-scaled\n",
    "        \n",
    "        new_cases_per_million = new_cases*0.096  #Calculate New per million \n",
    "        \n",
    "        \n",
    "        newcasesprediction.loc[len(newcasesprediction.index)] = [new_cases] #append new cases \n",
    "        smoothednew = newcasesprediction.rolling(window=7).mean()\n",
    "        new_cases_smoothed = float( smoothednew.iloc[6+i])\n",
    "        \n",
    "        new_cases_smoothed_pre_million= new_cases_smoothed * 0.096  #Calculate Smoothed Permillion New Cases \n",
    "        \n",
    "        \n",
    "        #Scale Back \n",
    "        \n",
    "        total_cases = total_cases * 5.80966e-06\n",
    "        new_cases = new_cases * 0.000301568\n",
    "        new_cases_smoothed = new_cases_smoothed * 0.000374211\n",
    "        total_cases_per_million = total_cases_per_million * 5.82583e-05\n",
    "        new_cases_per_million = new_cases_per_million * 0.00314326\n",
    "        new_cases_smoothed_pre_million = new_cases_smoothed_pre_million * 0.00389804\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        #Add New Day Values \n",
    "        Featnames = ['total_cases','new_cases','new_cases_smoothed','total_cases_per_million','new_cases_per_million','new_cases_smoothed_per_million']\n",
    "        featval = [total_cases,new_cases,new_cases_smoothed,total_cases_per_million,new_cases_per_million,new_cases_smoothed_pre_million]\n",
    "        dictionary = dict(zip(Featnames, featval))\n",
    "        usedval =[ dictionary[feature_list[0]] , dictionary[feature_list[1]] ,dictionary[feature_list[2]] , dictionary[feature_list[3]] ,dictionary[feature_list[4]] ]\n",
    "        \n",
    "        predictiondata.loc[len(predictiondata.index)] = usedval\n",
    "    \n",
    "\n",
    "    forecast = predictiondata[-(future):] #Save results in a dataframe \n",
    "    forecast = sc.inverse_transform(forecast)#Inverse Transform to get the actual cases \n",
    "    forecast = pd.DataFrame(forecast.round()) #Round results \n",
    "    forecast = forecast.set_index(index[seq_size:], 'Date').rename(columns={0: 'Prediction'})\n",
    "\n",
    "    forecast = pd.concat([forecast['Prediction'], inverseval['total_cases'][seq_size:]], axis=1 ,ignore_index=True) #Concate the two dfs \n",
    "\n",
    "    forecast=forecast.set_axis(['Prediction', 'Actual'], axis=1, inplace=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIND BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(mape):\n",
    "    mape = pd.DataFrame(mape)\n",
    "    min = mape.idxmin()\n",
    "    j = min[0]\n",
    "    best_model = keras.models.load_model(r\"Models/model_\" + str(j) + \".h5\")\n",
    "    print(\"Best Model is :model_\" + str(j) + \".h5\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLUSTER FUCK LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments(i, nodes, scaler, seq_size, epochs, n_features, train_generator, val_generator, validation_set,\n",
    "                train_set, inv_val, inv_test, dates ,lrate):\n",
    "    \n",
    "    experimentmodel = model_create(nodes, seq_size ,n_features , lrate)\n",
    "\n",
    "    experimentmodel = model_train(i, experimentmodel, train_generator, val_generator, epochs)  # Train Model\n",
    "\n",
    "    forecast = predict(experimentmodel, scaler, val_generator, validation_set, inv_val, train_set)\n",
    "    # plotprediction(forecast ,str(i))\n",
    "    \n",
    "    \n",
    "    ##################### Metrics ######################\n",
    "\n",
    "    mae_4 = mean_absolute_error(forecast['Actual'], forecast['Prediction'])\n",
    "    MAE_4.append(mae_4)\n",
    "\n",
    "    mape_4 = mean_absolute_percentage_error(forecast['Actual'], forecast['Prediction'])\n",
    "    MAPE_4.append(mape_4)\n",
    "\n",
    "    mse_4 = mean_squared_error(forecast['Actual'], forecast['Prediction'])\n",
    "    MSE_4.append(mse_4)\n",
    "\n",
    "    rmse_4 = mean_squared_error(forecast['Actual'], forecast['Prediction'], squared=False)\n",
    "    RMSE_4.append(rmse_4)\n",
    "\n",
    "    node.append(nodes)\n",
    "\n",
    "\n",
    "    mape_4_next_day = mean_absolute_percentage_error(forecast['Actual'][:1], forecast['Prediction'][:1])\n",
    "    MAPE_4_Next_day.append(mape_4_next_day)\n",
    " \n",
    "    mape_3days = mean_absolute_percentage_error(forecast['Actual'][:3], forecast['Prediction'][:3])\n",
    "    MAPE_4_3days.append(mape_3days)\n",
    "    \n",
    "    mape_7days = mean_absolute_percentage_error(forecast['Actual'][:7], forecast['Prediction'][:7])\n",
    "    MAPE_4_7days.append(mape_7days)\n",
    "    \n",
    "    Epochs.append(epochs)\n",
    "    Features.append(feature_list)\n",
    "\n",
    "    LR.append(lrate)\n",
    "        \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hyper(parameter1 , parameter2 , parameter3 , repetitions):\n",
    "    hp1 = list(product(parameter1 , parameter2 ))\n",
    "    Hyperparameters = list (product(hp1 , parameter3))\n",
    "    Hyperparameters= pd.DataFrame(Hyperparameters).rename(columns={0: \"A\", 1: \"Nodes\"})\n",
    "    \n",
    "    Hyperparameters[['Learning Rate' , 'Epochs']]= pd.DataFrame(Hyperparameters['A'].tolist(), index=Hyperparameters.index)\n",
    "    \n",
    "    Hyperparameters =Hyperparameters.drop(['A'], axis=1)\n",
    "    Hyperparameters=Hyperparameters.sort_values(by=['Nodes', 'Learning Rate' ,'Epochs' ])\n",
    "    Hyperparameters=pd.concat([Hyperparameters]*times)\n",
    "    \n",
    "    \n",
    "    Hyperparameters= list(Hyperparameters.itertuples(index=False, name=None))\n",
    "    \n",
    "    \n",
    "    return Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMTERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_size = 3\n",
    "times =10\n",
    "combos=5\n",
    "nodes=2\n",
    "lr = 0.0001\n",
    "epochs=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN LOOP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = []\n",
    "LR = []\n",
    "node = []\n",
    "MAE_4 = []\n",
    "MAPE_4 = []\n",
    "MSE_4 = []\n",
    "RMSE_4 = []\n",
    "MAPE_4_3days = []\n",
    "MAPE_4_7days = []\n",
    "MAPE_4_Next_day = []\n",
    "\n",
    "Features = []\n",
    "loc=\"owid-covid-data.csv\"\n",
    "Greece_total , titles =readdata(loc)\n",
    "flist = featcombos('cases', titles, combos)\n",
    "\n",
    "\n",
    "flist=flist*times\n",
    "flist=[ x for x in flist if \"total_cases\"  in x ]\n",
    "\n",
    "\n",
    "# flist=flist[:1]\n",
    "\n",
    "for i in range(len(flist)):\n",
    "    feature_list= flist[i]\n",
    "\n",
    "    feature_list = list(itertools.chain(feature_list))\n",
    "    n_features = len(feature_list)\n",
    "    \n",
    "    Greece_total['new_cases_smoothed']= Greece_total['new_cases'].rolling(window=7).mean()\n",
    "    Greece_total['new_deaths_smoothed']= Greece_total['new_deaths'].rolling(window=7).mean()\n",
    "    \n",
    "    Greece_total['new_cases_smoothed_per_million']= Greece_total['new_cases_smoothed']*0.096\n",
    "    Greece_total['new_deaths_smoothed_per_million']= Greece_total['new_deaths_smoothed']*0.096\n",
    "\n",
    "\n",
    "\n",
    "    dates,greece = createdata(Greece_total ,feature_list )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    train_set, validation_set, test_set = split_data( greece, seq_size)\n",
    "    \n",
    "    #Scaling \n",
    "    scaler = MinMaxScaler() \n",
    "    scaler.fit(train_set)\n",
    "    train_set=pd.DataFrame(scaler.transform(train_set))\n",
    "    train_set=train_set.set_axis(feature_list, axis=1, inplace=False)\n",
    "    \n",
    "    validation_set=pd.DataFrame(scaler.transform(validation_set))\n",
    "    validation_set=validation_set.set_axis(feature_list, axis=1, inplace=False)\n",
    "    \n",
    "    test_set=pd.DataFrame(scaler.transform(test_set))\n",
    "    test_set=test_set.set_axis(feature_list, axis=1, inplace=False)\n",
    "    \n",
    "    \n",
    "    train_generator, val_generator, test_generator = timeseries_gen(seq_size, n_features, train_set, validation_set,\n",
    "                                                                    test_set)\n",
    "    # a =train_generator[3]\n",
    "    \n",
    "    inv_train, inv_val, inv_test = inversesets(seq_size,feature_list, scaler, train_set, validation_set, test_set, greece,\n",
    "                                                            dates)\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters= Hyper(learning_rate, epochs, nodes ,times )\n",
    "\n",
    "\n",
    "    # nodes , lr , epochs = Hyperparameters[i]\n",
    "    experiments(i, nodes, scaler, seq_size, epochs, n_features, train_generator, val_generator,\n",
    "                      validation_set, train_set, inv_val, inv_test, dates , lr )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(\n",
    "    {'Feat':Features  ,'MAE_4': MAE_4, 'MAPE_4 1 Day': MAPE_4_Next_day,\n",
    "      'MAPE_4 3 Days': MAPE_4_3days,'MAPE_4 7 days': MAPE_4_7days, 'MAPE_4': MAPE_4, 'MSE_4': MSE_4, 'RMSE_4': RMSE_4 , 'Epochs' : Epochs})\n",
    "\n",
    "metrics=metrics.sort_values(by=['Feat']).reset_index(drop=True)\n",
    "\n",
    "metrics[['Feature 1','Feature 2', 'Feature 3']] = pd.DataFrame(metrics.Feat.tolist(), index= metrics.index)\n",
    "metrics1 = metrics.groupby(['Feature 1', 'Feature 2' , 'Feature 3']).mean()\n",
    " \n",
    "\n",
    "# #Save Results\n",
    "metrics.to_csv(\"Valdation_Results_for_\"+ str(len(feature_list)) +\".csv\", float_format=\"%.5f\",index=True, header=True)\n",
    "metrics1.to_csv(\"AverageValdation_Results_for_\"+ str(len(feature_list)) +\".csv\", float_format=\"%.5f\",index=True, header=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7456b8b16b80f58bc66af7790c179bcda12a832a7c60efc6ab6015df57e8528d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('pythonProject': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
