{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from itertools  import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS // THAT DONT CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## Telegram Bot   ###################################\n",
    "\n",
    "import requests\n",
    "\n",
    "def telegram_bot_sendtext(bot_message):\n",
    "    \n",
    "    bot_token = '2062474091:AAGp1GiSrNw7DRds4qwLHBOkZ_Do9HlQ5V8'\n",
    "    bot_chatID = '2013533042'\n",
    "    send_text = 'https://api.telegram.org/bot' + bot_token + '/sendMessage?chat_id=' + bot_chatID + '&parse_mode=Markdown&text=' + bot_message\n",
    "    \n",
    "    response = requests.get(send_text)\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "########################## Functions   ###################################\n",
    "\n",
    "def createdata(location , feature_list):\n",
    "    \n",
    "    data=pd.read_csv(location)\n",
    "    Greece= data[data.location =='Greece'].reset_index(drop='True')\n",
    "    Greece = Greece.dropna(how='all', axis=1)\n",
    "    Greece_total = Greece.iloc[7:498, 3:40].reset_index(drop='True')\n",
    "    \n",
    "    Greece=Greece_total[(feature_list)]\n",
    "    Greece[\"date\"] = Greece_total['date']\n",
    "    Greece=Greece.dropna(axis=0)\n",
    "    dates=pd.DataFrame(Greece['date']).reset_index(drop=True)\n",
    "    Greece=Greece[(feature_list)].reset_index(drop=True)\n",
    "    \n",
    "    return dates , Greece , Greece_total\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def split_data(data, sequence):\n",
    "    train_set = data[:355].reset_index(drop=True)\n",
    "    validation_set = data[355 - sequence:369].reset_index(drop=True)\n",
    "    test_set = data[369 - sequence:].reset_index(drop=True) \n",
    "    \n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "\n",
    "def timeseries_gen(seq_size, n_features, train, val, test):\n",
    "    # Train Set\n",
    "    train_generator = TimeseriesGenerator(train, train.iloc[:, 0], length=seq_size, batch_size=1)\n",
    "    print(\"Total number of samples in the original training data = \", len(train))\n",
    "    print(\"Total number of samples in the generated training data = \", len(train_generator))\n",
    "\n",
    "\n",
    "    # Validation Set\n",
    "    val_generator = TimeseriesGenerator(val, val.iloc[:, 0], length=seq_size, batch_size=1)\n",
    "    print(\"Total number of samples in the original validation data = \", len(val))\n",
    "    print(\"Total number of samples in the validation data = \", len(val_generator))\n",
    "\n",
    "    # Test Set\n",
    "    test_generator = TimeseriesGenerator(test, test.iloc[:, 0], length=seq_size, batch_size=1)\n",
    "    print(\"Total number of samples in the original test data = \", len(test))\n",
    "    print(\"Total number of samples in the generated test data = \", len(test_generator))\n",
    "    return train_generator, val_generator, test_generator\n",
    "\n",
    "def plotloss(mod, name=\"\"):\n",
    "    plt.figure(figsize=[12,10] , dpi=140 )\n",
    "    loss = mod.history['loss']\n",
    "    val_loss = mod.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "#     plt.savefig(\"Plots\\loss_model\" + name +\".jpeg\"  )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotprediction(ypredict , name=\"\"):\n",
    "    plt.figure(figsize=[12,10] , dpi=140 )\n",
    "    plt.plot(ypredict.index, ypredict.iloc[:, 0], 'y', label='Prediction ')\n",
    "    plt.plot(ypredict.index, ypredict.iloc[:, 1], 'r', label='Actual ')\n",
    "    plt.title('Predicted vs  Actual Cases in Greece for ' +str(len(ypredict)) + ' days')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cases')\n",
    "    plt.legend()\n",
    "#     plt.savefig(\"Plots\\pred\" + name +\".jpeg\"  )\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def inversesets(sequence,feature_list, sc, trainset, validationset, testset, ogdata, dates):\n",
    "    \n",
    "    drange =dates.loc[0]\n",
    "    drange=pd.to_datetime(drange[\"date\"])\n",
    "    date_index = pd.date_range(drange , periods=len(dates), freq='D')\n",
    "\n",
    "    \n",
    "    \n",
    "    set1 = pd.DataFrame(sc.inverse_transform(trainset),index=date_index[0:len(trainset)])\n",
    "\n",
    "    set1=set1.set_axis(feature_list, axis=1, inplace=False)\n",
    "    \n",
    "    set2 = pd.DataFrame(sc.inverse_transform(validationset),index=date_index[len(trainset) - sequence:len(trainset) + len(validationset) - sequence])\n",
    "    set2=set2.set_axis(feature_list, axis=1, inplace=False)\n",
    "\n",
    "    set3 = pd.DataFrame(sc.inverse_transform(testset),index=date_index[-len(testset):])\n",
    "    set3=set3.set_axis(feature_list, axis=1, inplace=False)\n",
    "    return set1, set2, set3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "MODEL"
    ]
   },
   "source": [
    "MODEL CREATION , TRAINING , PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_create(nodes, seq_size , features,lrate):\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(nodes, activation='relu', return_sequences=True, input_shape=(seq_size, features)))\n",
    "    # model.add(Dropout(nodes))\n",
    "    model.add(LSTM(nodes , return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "TRAIN"
    ]
   },
   "source": [
    "TRAIN NO PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(i, model, traingenerator, valgenerator, ep):\n",
    "    history = model.fit(traingenerator, validation_data=valgenerator, epochs=ep, verbose=1)\n",
    "    model.save('Models\\model_' + str(i) + '.h5', overwrite=True)\n",
    "#     plotloss(history,str(i))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sc, valgenerator, validation_set, inverseval, trainset ):\n",
    "\n",
    "\n",
    "    # Forecast   Predict using a for loop\n",
    "    index = inverseval.index\n",
    "    predictiondata = pd.DataFrame(inverseval[:seq_size])  # Empty list to populate later with predictions\n",
    "    predictiondata = pd.DataFrame(trainset[-seq_size:]).reset_index(drop=True)\n",
    "    current_batch = trainset[-seq_size:]\n",
    "    forecast = pd.DataFrame()\n",
    "\n",
    "    # Predict future, beyond test dates\n",
    "    future = len(validation_set) - seq_size  # Days\n",
    "    for i in range(future):\n",
    "                \n",
    "        current_batch = predictiondata[i:seq_size + i] #Create input for LSTM (Based on sequence size )\n",
    "\n",
    "        current_batch = current_batch.to_numpy()  #Input to array \n",
    "\n",
    "        current_batch = current_batch.reshape(1, seq_size, n_features)  # Reshape\n",
    "\n",
    "        ### Prediction ##\n",
    "        \n",
    "        current_pred = model.predict(current_batch) # Make a prediction \n",
    "        current_pred = float(current_pred[0]) #Convert Prediction to integer \n",
    "        predictiondata.loc[len(predictiondata.index)] = [current_pred]   \n",
    "\n",
    "    forecast = predictiondata[-(future):] #Save results in a dataframe \n",
    "    forecast = sc.inverse_transform(forecast)#Inverse Transform to get the actual cases \n",
    "    forecast = pd.DataFrame(forecast.round()) #Round results \n",
    "    forecast = forecast.set_index(index[seq_size:], 'Date').rename(columns={0: 'Prediction'})\n",
    "\n",
    "    forecast = pd.concat([forecast['Prediction'], inverseval['total_cases'][seq_size:]], axis=1 ,ignore_index=True) #Concate the two dfs \n",
    "\n",
    "    forecast=forecast.set_axis(['Prediction', 'Actual'], axis=1, inplace=False)\n",
    "    \n",
    "    \n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLUSTER FUCK LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments(times, nodes, scaler, seq_size, epochs, n_features, train_generator, val_generator, validation_set,\n",
    "                train_set, inv_val, inv_test, dates ,lrate):\n",
    "    \n",
    "    experimentmodel = model_create(nodes, seq_size ,n_features , lrate)\n",
    "\n",
    "    experimentmodel = model_train(i, experimentmodel, train_generator, val_generator, epochs)  # Train Model\n",
    "\n",
    "    forecast = predict(experimentmodel, scaler, val_generator, validation_set, inv_val, train_set)\n",
    "#     plotprediction(forecast ,str(i))\n",
    "    \n",
    "    \n",
    "    ##################### Metrics ######################\n",
    "\n",
    "    mae_4 = mean_absolute_error(forecast['Actual'], forecast['Prediction'])\n",
    "    MAE_4.append(mae_4)\n",
    "\n",
    "    mape_4 = mean_absolute_percentage_error(forecast['Actual'], forecast['Prediction'])\n",
    "    MAPE_4.append(mape_4)\n",
    "\n",
    "    mse_4 = mean_squared_error(forecast['Actual'], forecast['Prediction'])\n",
    "    MSE_4.append(mse_4)\n",
    "\n",
    "    rmse_4 = mean_squared_error(forecast['Actual'], forecast['Prediction'], squared=False)\n",
    "    RMSE_4.append(rmse_4)\n",
    "\n",
    "    node.append(nodes)\n",
    "\n",
    "\n",
    "    mape_4_next_day = mean_absolute_percentage_error(forecast['Actual'][:1], forecast['Prediction'][:1])\n",
    "    MAPE_4_Next_day.append(mape_4_next_day)\n",
    " \n",
    "    mape_3days = mean_absolute_percentage_error(forecast['Actual'][:3], forecast['Prediction'][:3])\n",
    "    MAPE_4_3days.append(mape_3days)\n",
    "    \n",
    "    mape_7days = mean_absolute_percentage_error(forecast['Actual'][:7], forecast['Prediction'][:7])\n",
    "    MAPE_4_7days.append(mape_7days)\n",
    "    \n",
    "    Epochs.append(epochs)\n",
    "    LR.append(lrate)\n",
    "        \n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hyper(parameter1 , parameter2 , parameter3 , repetitions):\n",
    "    hp1 = list(product(parameter1 , parameter2 ))\n",
    "    Hyperparameters = list (product(hp1 , parameter3))\n",
    "    Hyperparameters= pd.DataFrame(Hyperparameters).rename(columns={0: \"A\", 1: \"Nodes\"})\n",
    "    \n",
    "    Hyperparameters[['Learning Rate' , 'Epochs']]= pd.DataFrame(Hyperparameters['A'].tolist(), index=Hyperparameters.index)\n",
    "    \n",
    "    Hyperparameters =Hyperparameters.drop(['A'], axis=1)\n",
    "    Hyperparameters=Hyperparameters.sort_values(by=['Nodes', 'Learning Rate' ,'Epochs' ])\n",
    "    Hyperparameters=pd.concat([Hyperparameters]*times)\n",
    "    \n",
    "    \n",
    "    Hyperparameters= list(Hyperparameters.itertuples(index=False, name=None))\n",
    "    \n",
    "    \n",
    "    return Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMTERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Windeos_loc=\"owid-covid-data.csv\"\n",
    "feature_list=[\"total_cases\"]\n",
    "n_features = len(feature_list)\n",
    "seq_size = 3\n",
    "\n",
    "times =1 #For each experiment\n",
    "lr = 0.1\n",
    "epochs = 2\n",
    "nodes = 88\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN LOOP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arisdourdounas/opt/anaconda3/envs/pythonProject/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in the original training data =  355\n",
      "Total number of samples in the generated training data =  352\n",
      "Total number of samples in the original validation data =  17\n",
      "Total number of samples in the validation data =  14\n",
      "Total number of samples in the original test data =  125\n",
      "Total number of samples in the generated test data =  122\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 3, 88)             31680     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 88)                62304     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 89        \n",
      "=================================================================\n",
      "Total params: 94,073\n",
      "Trainable params: 94,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train for 352 steps, validate for 14 steps\n",
      "Epoch 1/2\n",
      "352/352 [==============================] - 7s 20ms/step - loss: 0.0848 - val_loss: 0.0592\n",
      "Epoch 2/2\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 7.3311e-04 - val_loss: 0.0018\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dates,greece , Greece_total =createdata(Windeos_loc,feature_list)\n",
    "train_set, validation_set, test_set = split_data( greece, seq_size)\n",
    "scaler = MinMaxScaler() \n",
    "scaler.fit(train_set)\n",
    "train_set=pd.DataFrame(scaler.transform(train_set))\n",
    "train_set=train_set.set_axis(feature_list, axis=1, inplace=False)\n",
    "validation_set=pd.DataFrame(scaler.transform(validation_set))\n",
    "validation_set=validation_set.set_axis(feature_list, axis=1, inplace=False)\n",
    "test_set=pd.DataFrame(scaler.transform(test_set))\n",
    "test_set=test_set.set_axis(feature_list, axis=1, inplace=False)\n",
    "train_generator, val_generator, test_generator = timeseries_gen(seq_size, n_features, train_set, validation_set, test_set)\n",
    "inv_train, inv_val, inv_test = inversesets(seq_size,feature_list, scaler, train_set, validation_set, test_set, greece,dates)\n",
    "\n",
    "\n",
    "Epochs = []\n",
    "LR = []\n",
    "node = []\n",
    "MAE_4 = []\n",
    "MAPE_4 = []\n",
    "MSE_4 = []\n",
    "RMSE_4 = []\n",
    "MAPE_4_3days = []\n",
    "MAPE_4_7days = []\n",
    "MAPE_4_Next_day = []\n",
    "\n",
    "\n",
    "for i in  range(times):\n",
    "    experiments(i, nodes, scaler, seq_size, epochs, n_features, train_generator, val_generator,validation_set, train_set, inv_val, inv_test, dates , lr )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame({'MAE_4': MAE_4, 'MAPE_4 1 Day': MAPE_4_Next_day,\n",
    "     'MAPE_4 3 Days': MAPE_4_3days,'MAPE_4 7 days': MAPE_4_7days, 'MAPE_4': MAPE_4, 'MSE_4': MSE_4, 'RMSE_4': RMSE_4, 'Nodes': node , 'Learning Rate' : LR , 'Epochs' : Epochs})\n",
    "\n",
    "# metrics =metrics.append( metrics.groupby(['Nodes' , 'Learning Rate'  , 'Epochs']).mean())\n",
    "metrics = metrics.groupby(['Nodes' , 'Learning Rate'  , 'Epochs']).mean()\n",
    "metrics.to_csv(\"Results.csv\", float_format=\"%.5f\",index=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pythonProject]",
   "language": "python",
   "name": "conda-env-pythonProject-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
