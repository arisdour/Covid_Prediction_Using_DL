{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import callbacks\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from itertools  import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS // THAT DONT CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdata(location , feature_list):\n",
    "    \n",
    "    data=pd.read_csv(location)\n",
    "    Greece= data[data.location =='Greece'].reset_index(drop='True')\n",
    "    Greece = Greece.dropna(how='all', axis=1)\n",
    "    Greece_total = Greece.iloc[7:498, 3:40].reset_index(drop='True')\n",
    "    \n",
    "    Greece=Greece_total[(feature_list)]\n",
    "    Greece[\"date\"] = Greece_total['date']\n",
    "    Greece=Greece.dropna(axis=0)\n",
    "    dates=pd.DataFrame(Greece['date']).reset_index(drop=True)\n",
    "    Greece=Greece[(feature_list)].reset_index(drop=True)\n",
    "    \n",
    "    return dates , Greece , Greece_total\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def split_data(data, sequence):\n",
    "    train_set = data[:355].reset_index(drop=True)\n",
    "    validation_set = data[355 - sequence:369].reset_index(drop=True)\n",
    "    test_set = data[369 - sequence:].reset_index(drop=True) \n",
    "    \n",
    "    return train_set, validation_set, test_set\n",
    "\n",
    "\n",
    "def timeseries_gen(seq_size, n_features, train, val, test):\n",
    "    # Train Set\n",
    "    train_generator = TimeseriesGenerator(train, train.iloc[:, 0], length=seq_size, batch_size=1)\n",
    "    print(\"Total number of samples in the original training data = \", len(train))\n",
    "    print(\"Total number of samples in the generated training data = \", len(train_generator))\n",
    "\n",
    "\n",
    "    # Validation Set\n",
    "    val_generator = TimeseriesGenerator(val, val.iloc[:, 0], length=seq_size, batch_size=1)\n",
    "    print(\"Total number of samples in the original validation data = \", len(val))\n",
    "    print(\"Total number of samples in the validation data = \", len(val_generator))\n",
    "\n",
    "    # Test Set\n",
    "    test_generator = TimeseriesGenerator(test, test.iloc[:, 0], length=seq_size, batch_size=1)\n",
    "    print(\"Total number of samples in the original test data = \", len(test))\n",
    "    print(\"Total number of samples in the generated test data = \", len(test_generator))\n",
    "    return train_generator, val_generator, test_generator\n",
    "\n",
    "\n",
    "def plotloss(mod, name=\"\"):\n",
    "    plt.figure(figsize=[12,10] , dpi=140 )\n",
    "    loss = mod.history['loss']\n",
    "    val_loss = mod.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    # plt.savefig(\"Plots\\loss_model\" + name +\".jpeg\"  )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotprediction(ypredict , name=\"\"):\n",
    "    plt.figure(figsize=[12,10] , dpi=140 )\n",
    "    plt.plot(ypredict.index, ypredict.iloc[:, 0], 'y', label='Prediction ')\n",
    "    plt.plot(ypredict.index, ypredict.iloc[:, 1], 'r', label='Actual ')\n",
    "    plt.title('Predicted vs  Actual Cases in Greece for ' +str(len(ypredict)) + ' days')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cases')\n",
    "    plt.legend()\n",
    "    # plt.savefig(\"Plots\\pred\" + name +\".jpeg\"  )\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "\n",
    "def inversesets(sequence,feature_list, sc, trainset, validationset, testset, ogdata, dates):\n",
    "    \n",
    "    drange =dates.loc[0]\n",
    "    drange=pd.to_datetime(drange[\"date\"])\n",
    "    date_index = pd.date_range(drange , periods=len(dates), freq='D')\n",
    "\n",
    "    \n",
    "    \n",
    "    set1 = pd.DataFrame(sc.inverse_transform(trainset),index=date_index[0:len(trainset)])\n",
    "\n",
    "    set1=set1.set_axis(feature_list, axis=1, inplace=False)\n",
    "    \n",
    "    set2 = pd.DataFrame(sc.inverse_transform(validationset),index=date_index[len(trainset) - sequence:len(trainset) + len(validationset) - sequence])\n",
    "    set2=set2.set_axis(feature_list, axis=1, inplace=False)\n",
    "\n",
    "    set3 = pd.DataFrame(sc.inverse_transform(testset),index=date_index[-len(testset):])\n",
    "    set3=set3.set_axis(feature_list, axis=1, inplace=False)\n",
    "    return set1, set2, set3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "MODEL"
    ]
   },
   "source": [
    "MODEL CREATION , TRAINING , PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_create(nodes1 ,nodes2 , seq_size , features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(nodes1, activation='relu', return_sequences=True, input_shape=(seq_size, features)))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(LSTM(nodes2, return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "TRAIN"
    ]
   },
   "source": [
    "TRAIN NO PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(i, model, traingenerator, valgenerator, ep):\n",
    "    # earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", mode =\"min\", patience = 5, restore_best_weights = True)\n",
    "\n",
    "\n",
    "    history = model.fit(traingenerator, validation_data=valgenerator, epochs=ep ,verbose=1)\n",
    "    # history = model.fit(traingenerator, validation_data=valgenerator, epochs=ep,batch_size= 1 ,verbose=1,callbacks =[earlystopping])\n",
    "\n",
    "    \n",
    "    # model.save('Models\\model_' + str(i) + '.h5', overwrite=True)\n",
    "    # plotloss(history,str(i))\n",
    "    # avep.append( len(history.history['loss']))\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sc, valgenerator, validation_set, inverseval, trainset ):\n",
    "    \n",
    "\n",
    "    # Forecast   Predict using a for loop\n",
    "    index = inverseval.index\n",
    "    predictiondata = pd.DataFrame(inverseval[:seq_size])  # Empty list to populate later with predictions\n",
    "    predictiondata = pd.DataFrame(trainset[-seq_size:]).reset_index(drop=True)\n",
    "    current_batch = trainset[-seq_size:]\n",
    "    forecast = pd.DataFrame()\n",
    "\n",
    "    # Predict future, beyond test dates\n",
    "    future = len(validation_set) - seq_size  # Days\n",
    "    for i in range(future):\n",
    "                \n",
    "        current_batch = predictiondata[i:seq_size + i] #Create input for LSTM (Based on sequence size )\n",
    "\n",
    "        current_batch = current_batch.to_numpy()  #Input to array \n",
    "\n",
    "        current_batch = current_batch.reshape(1, seq_size, n_features)  # Reshape\n",
    "\n",
    "        ### Prediction ##\n",
    "        \n",
    "        current_pred = model.predict(current_batch) # Make a prediction \n",
    "        current_pred = float(current_pred[0]) #Convert Prediction to integer \n",
    "        predictiondata.loc[len(predictiondata.index)] = [current_pred]   \n",
    "\n",
    "    forecast = predictiondata[-(future):] #Save results in a dataframe \n",
    "    forecast = sc.inverse_transform(forecast)#Inverse Transform to get the actual cases \n",
    "    forecast = pd.DataFrame(forecast.round()) #Round results \n",
    "    forecast = forecast.set_index(index[seq_size:], 'Date').rename(columns={0: 'Prediction'})\n",
    "\n",
    "    forecast = pd.concat([forecast['Prediction'], inverseval['total_cases'][seq_size:]], axis=1 ,ignore_index=True) #Concate the two dfs \n",
    "\n",
    "    forecast=forecast.set_axis(['Prediction', 'Actual'], axis=1, inplace=False)\n",
    "    \n",
    "    \n",
    "    return forecast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hyper(parameter1 , parameter2 , parameter3 , repetitions):\n",
    "    hp1 = list(product(parameter1 , parameter2 ))\n",
    "    Hyperparameters = list (product(hp1 , parameter3))\n",
    "    Hyperparameters= pd.DataFrame(Hyperparameters).rename(columns={0: \"A\", 1: \"Nodes\"})\n",
    "    \n",
    "    Hyperparameters[['Learning Rate' , 'Epochs']]= pd.DataFrame(Hyperparameters['A'].tolist(), index=Hyperparameters.index)\n",
    "    \n",
    "    Hyperparameters =Hyperparameters.drop(['A'], axis=1)\n",
    "    Hyperparameters=Hyperparameters.sort_values(by=['Nodes', 'Learning Rate' ,'Epochs' ])\n",
    "    Hyperparameters=pd.concat([Hyperparameters]*times)\n",
    "    \n",
    "    \n",
    "    Hyperparameters= list(Hyperparameters.itertuples(index=False, name=None))\n",
    "    \n",
    "    \n",
    "    return Hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLUSTER FUCK LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def experiments(times, nodes1,nodes2, scaler, seq_size, epochs, n_features, train_generator, val_generator, validation_set,\n",
    "                train_set, inv_val, inv_test, dates ):\n",
    "    \n",
    "    experimentmodel = model_create(nodes1,nodes2, seq_size ,n_features )\n",
    "\n",
    "    experimentmodel = model_train(i, experimentmodel, train_generator, val_generator, epochs)  # Train Model\n",
    "\n",
    "    forecast = predict(experimentmodel, scaler, val_generator, validation_set, inv_val, train_set)\n",
    "    # plotprediction(forecast ,str(i))\n",
    "    \n",
    "    \n",
    "    ##################### Metrics ######################\n",
    "\n",
    "    mae_4 = mean_absolute_error(forecast['Actual'], forecast['Prediction'])\n",
    "    MAE_4.append(mae_4)\n",
    "\n",
    "    mape_4 = mean_absolute_percentage_error(forecast['Actual'], forecast['Prediction'])\n",
    "    MAPE_4.append(mape_4)\n",
    "\n",
    "    mse_4 = mean_squared_error(forecast['Actual'], forecast['Prediction'])\n",
    "    MSE_4.append(mse_4)\n",
    "\n",
    "    rmse_4 = mean_squared_error(forecast['Actual'], forecast['Prediction'], squared=False)\n",
    "    RMSE_4.append(rmse_4)\n",
    "\n",
    "    node.append(nodes)\n",
    "\n",
    "\n",
    "    mape_4_next_day = mean_absolute_percentage_error(forecast['Actual'][:1], forecast['Prediction'][:1])\n",
    "    MAPE_4_Next_day.append(mape_4_next_day)\n",
    " \n",
    "    mape_3days = mean_absolute_percentage_error(forecast['Actual'][:3], forecast['Prediction'][:3])\n",
    "    MAPE_4_3days.append(mape_3days)\n",
    "    \n",
    "    mape_7days = mean_absolute_percentage_error(forecast['Actual'][:7], forecast['Prediction'][:7])\n",
    "    MAPE_4_7days.append(mape_7days)\n",
    "    \n",
    "    Epochs.append(epochs)\n",
    "    Nodes1.append(nodes1)\n",
    "    Nodes2.append(nodes2)\n",
    "\n",
    "        \n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMTERS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Windeos_loc=\"owid-covid-data.csv\"\n",
    "feature_list=[\"total_cases\"]\n",
    "n_features = len(feature_list)\n",
    "seq_size = 3\n",
    "\n",
    "times = 10 #For each experiment\n",
    "\n",
    "\n",
    "\n",
    "nodes_0 = [18,20,22,25]\n",
    "\n",
    "nodes_1 =[30,35,44,59,88] \n",
    "nodes = [18,20,22,25,30,35,44,59,88]\n",
    "\n",
    "epochs = [5, 10, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(mape):\n",
    "    mape = pd.DataFrame(mape)\n",
    "    min = mape.idxmin()\n",
    "    j = min[0]\n",
    "    best_model = keras.models.load_model(r\"Models\\model_\" + str(j) + \".h5\")\n",
    "    print(\"Best Model is :model_\" + str(j) + \".h5\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN LOOP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Hyperparameters= Hyper(nodes_1, nodes , epochs,times )\n",
    "\n",
    "dates,greece , Greece_total =createdata(Windeos_loc,feature_list)\n",
    "train_set, validation_set, test_set = split_data( greece, seq_size)\n",
    "scaler = MinMaxScaler() \n",
    "scaler.fit(train_set)\n",
    "train_set=pd.DataFrame(scaler.transform(train_set))\n",
    "train_set=train_set.set_axis(feature_list, axis=1, inplace=False)\n",
    "validation_set=pd.DataFrame(scaler.transform(validation_set))\n",
    "validation_set=validation_set.set_axis(feature_list, axis=1, inplace=False)\n",
    "test_set=pd.DataFrame(scaler.transform(test_set))\n",
    "test_set=test_set.set_axis(feature_list, axis=1, inplace=False)\n",
    "train_generator, val_generator, test_generator = timeseries_gen(seq_size, n_features, train_set, validation_set, test_set)\n",
    "inv_train, inv_val, inv_test = inversesets(seq_size,feature_list, scaler, train_set, validation_set, test_set, greece,dates)\n",
    "\n",
    "avep=[]\n",
    "Epochs = []\n",
    "LR = []\n",
    "node = []\n",
    "MAE_4 = []\n",
    "MAPE_4 = []\n",
    "MSE_4 = []\n",
    "RMSE_4 = []\n",
    "MAPE_4_3days = []\n",
    "MAPE_4_7days = []\n",
    "MAPE_4_Next_day = []\n",
    "Nodes1 = []\n",
    "Nodes2 = []\n",
    "\n",
    "\n",
    "for i in  range(10):\n",
    "    epochs , firstnodes , secondnodes = 5 , 30  , 18\n",
    "    experiments(i, firstnodes , secondnodes, scaler, seq_size, epochs, n_features, train_generator, val_generator,validation_set, train_set, inv_val, inv_test, dates )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame({'MAE_4': MAE_4, 'MAPE_4 1 Day': MAPE_4_Next_day,\n",
    "     'MAPE_4 3 Days': MAPE_4_3days,'MAPE_4 7 days': MAPE_4_7days, 'MAPE_4': MAPE_4, 'MSE_4': MSE_4, 'RMSE_4': RMSE_4, 'Epochs' : Epochs , 'Layer 1' : Nodes1 , 'Layer 2': Nodes2})\n",
    "\n",
    "# metrics =metrics.append( metrics.groupby(['Nodes' , 'Learning Rate'  , 'Epochs']).mean())\n",
    "metrics = metrics.groupby(['Layer 1' ,'Layer 2' , 'Epochs']).mean()\n",
    "metrics.to_csv(\"Results_for_stacked.csv\", float_format=\"%.5f\",index=True, header=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodel = find_best_model(MAPE_4)\n",
    "\n",
    "\n",
    "average_epochs = round(sum(avep)/len(avep))\n",
    "\n",
    "bestmodel.fit_generator(val_generator, epochs=average_epochs, verbose=1) \n",
    "bestmodel.save(r\"Models\\Final_model_for_\"+ str(feature_list) + \".h5\")\n",
    "\n",
    "forecastf = predict(bestmodel, scaler, test_generator, test_set, inv_test, validation_set )\n",
    "\n",
    "plotprediction(forecastf[:7] , \"iction_7_day_prediction\")\n",
    "plotprediction(forecastf[:14] , \"iction_14_day_prediction\")\n",
    "plotprediction(forecastf[:30] , \"iction_30_day_prediction\")\n",
    "plotprediction(forecastf[:60] , \"iction_60_day_prediction\")\n",
    "plotprediction(forecastf[:90] , \"iction_90_day_prediction\")\n",
    "\n",
    "Days_7= []\n",
    "Days_14= []\n",
    "Days_30= []\n",
    "Days_60= []\n",
    "Days_90= []\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "mae = mean_absolute_error(forecastf['Actual'], forecastf['Prediction'])\n",
    "mae= float(\"{:.3f}\".format(mae))\n",
    "Days_90.append(mae)\n",
    "\n",
    "mae_7days = mean_absolute_error(forecastf['Actual'][:7], forecastf['Prediction'][:7])\n",
    "mae_7days= float(\"{:.3f}\".format(mae_7days))\n",
    "Days_7.append(mae_7days)\n",
    "\n",
    "\n",
    "mae_14days = mean_absolute_error(forecastf['Actual'][:14], forecastf['Prediction'][:14])\n",
    "mae_14days= float(\"{:.3f}\".format(mae_14days))\n",
    "Days_14.append(mae_14days)\n",
    "\n",
    "mae_30days = mean_absolute_error(forecastf['Actual'][:30], forecastf['Prediction'][:30])\n",
    "mae_30days= float(\"{:.3f}\".format(mae_30days))\n",
    "Days_30.append(mae_30days)\n",
    "\n",
    "mae_60days = mean_absolute_error(forecastf['Actual'][:60], forecastf['Prediction'][:60])\n",
    "mae_60days= float(\"{:.3f}\".format(mae_60days))\n",
    "Days_60.append(mae_60days)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "mape = mean_absolute_percentage_error(forecastf['Actual'], forecastf['Prediction'])\n",
    "mape= float(\"{:.3f}\".format(mape))\n",
    "Days_90.append(mape)\n",
    "\n",
    "\n",
    "mape_7days = mean_absolute_percentage_error(forecastf['Actual'][:7], forecastf['Prediction'][:7])\n",
    "mape_7days= float(\"{:.3f}\".format(mape_7days))\n",
    "Days_7.append(mape_7days)\n",
    "\n",
    "                  \n",
    "mape_14days = mean_absolute_percentage_error(forecastf['Actual'][:14], forecastf['Prediction'][:14])\n",
    "mape_14days= float(\"{:.3f}\".format(mape_14days))\n",
    "Days_14.append(mape_14days)\n",
    "\n",
    "\n",
    "mape_30days = mean_absolute_percentage_error(forecastf['Actual'][:30], forecastf['Prediction'][:30])\n",
    "mape_30days= float(\"{:.3f}\".format(mape_30days))\n",
    "Days_30.append(mape_30days)\n",
    "\n",
    "\n",
    "mape_60days = mean_absolute_percentage_error(forecastf['Actual'][:60], forecastf['Prediction'][:60])\n",
    "mape_60days= float(\"{:.3f}\".format(mape_60days))\n",
    "Days_60.append(mape_60days)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "mse = mean_squared_error(forecastf['Actual'], forecastf['Prediction'])\n",
    "mse= float(\"{:.3f}\".format(mse))\n",
    "Days_90.append(mse)\n",
    "\n",
    "\n",
    "mse_7days = mean_squared_error(forecastf['Actual'][:7], forecastf['Prediction'][:7])\n",
    "mse_7days= float(\"{:.3f}\".format(mse_7days))\n",
    "Days_7.append(mse_7days)\n",
    "\n",
    "mse_14days = mean_squared_error(forecastf['Actual'][:14], forecastf['Prediction'][:14])\n",
    "mse_14days= float(\"{:.3f}\".format(mse_14days))\n",
    "Days_14.append(mse_14days)\n",
    "\n",
    "\n",
    "mse_30days = mean_squared_error(forecastf['Actual'][:30], forecastf['Prediction'][:30])\n",
    "mse_30days= float(\"{:.3f}\".format(mse_30days))\n",
    "Days_30.append(mse_30days)\n",
    "\n",
    "\n",
    "mse_60days = mean_squared_error(forecastf['Actual'][:60], forecastf['Prediction'][:60])\n",
    "mse_60days= float(\"{:.3f}\".format(mse_60days))\n",
    "Days_60.append(mse_60days)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "rmse = mean_squared_error(forecastf['Actual'], forecastf['Prediction'], squared=False)\n",
    "rmse= float(\"{:.3f}\".format(rmse))\n",
    "Days_90.append(rmse)\n",
    "\n",
    "\n",
    "rmse_7days = mean_squared_error(forecastf['Actual'][:7], forecastf['Prediction'][:7] , squared=False)\n",
    "rmse_7days= float(\"{:.3f}\".format(rmse_7days))\n",
    "Days_7.append(rmse_7days)\n",
    "\n",
    "\n",
    "rmse_14days = mean_squared_error(forecastf['Actual'][:14], forecastf['Prediction'][:14] , squared=False)\n",
    "rmse_14days= float(\"{:.3f}\".format(rmse_14days))\n",
    "Days_14.append(rmse_14days)\n",
    "\n",
    "\n",
    "rmse_30days = mean_squared_error(forecastf['Actual'][:30], forecastf['Prediction'][:30] , squared=False)\n",
    "rmse_30days= float(\"{:.3f}\".format(rmse_30days))\n",
    "Days_30.append(rmse_30days)\n",
    "\n",
    "\n",
    "rmse_60days = mean_squared_error(forecastf['Actual'][:60], forecastf['Prediction'][:60] , squared=False)\n",
    "rmse_60days= float(\"{:.3f}\".format(rmse_60days))\n",
    "Days_60.append(rmse_60days)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "\n",
    "Names = ['MAE' , 'MAPE' , 'MSE'  , 'RMSE']\n",
    "finalresults=pd.DataFrame({\" 7 Days\" :Days_7, \" 14 Days\" :Days_14, \" 30 Days\" :Days_30,\" 60 Days\" :Days_60,\" 90 Days\":Days_90  , 'NAMES':Names })\n",
    "finalresults=finalresults.set_index(['NAMES'])\n",
    "\n",
    "finalresults.to_csv(\"Results\\Final_Results_for_\" + str(feature_list) +\".csv\", float_format=\"%.3f\",index=True, header=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7456b8b16b80f58bc66af7790c179bcda12a832a7c60efc6ab6015df57e8528d"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pythonProject]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
